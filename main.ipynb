{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 19:31:34.941928: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#import fasttext\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3195/675437588.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  train = train.drop(['TR','AG'],1)\n",
      "/tmp/ipykernel_3195/675437588.py:6: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  test = test.drop(['TR','AG'],1)\n",
      "/tmp/ipykernel_3195/675437588.py:7: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  val = val.drop(['TR','AG'],1)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/hateval2019_en_train.csv')\n",
    "test = pd.read_csv('data/hateval2019_en_test.csv')\n",
    "val = pd.read_csv('data/hateval2019_en_dev.csv')\n",
    "\n",
    "train = train.drop(['TR','AG'],1)\n",
    "test = test.drop(['TR','AG'],1)\n",
    "val = val.drop(['TR','AG'],1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:xlabel='HS', ylabel='count'>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=train,x='HS')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unbalanced but not massively"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fasttext model. tokenizer->fast text embedding ->LSTM->Linear->linear->sigmoid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.regularizers import L1,L2, l1_l2\n",
    "import io"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def normalize_tweet(text):\n",
    "    \"\"\"\n",
    "    Removes hashtags, @s, links, and punctuation\n",
    "    :param text:Text to be cleaned\n",
    "    :return: text with mentions, hashtages, and urls removes\n",
    "    \"\"\"\n",
    "    processed_text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www|t\\.)\\S+\", \"\", text)\n",
    "    processed_text = re.sub(r\"(?:\\.|,|\\?|-)\", \" \", processed_text)\n",
    "    processed_text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www|\\.com)\", \"\", processed_text)\n",
    "    processed_text = re.sub(r'[^\\w\\s]', '', processed_text)\n",
    "    processed_text = \" \".join(processed_text.split())\n",
    "    return processed_text\n",
    "\n",
    "def x_y_split(data):\n",
    "    \"\"\"splits and X and y from dataframe\n",
    "\n",
    "    Args:\n",
    "        data:dataframe to split from\n",
    "\n",
    "    Returns:\n",
    "        tuple:X data, y data\n",
    "    \"\"\"\n",
    "    X = data['text']\n",
    "    X = X.apply(normalize_tweet)\n",
    "    y = data['HS']\n",
    "    return X, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Hurray saving us in so many ways LockThemUp Bu...\n",
      "1       Why would young fighting age men be the vast m...\n",
      "2       Illegals Dump their Kids at the border like Ro...\n",
      "3       NY Times Nearly All White States Pose an Array...\n",
      "4       Orban in Brussels European leaders are ignorin...\n",
      "                              ...                        \n",
      "8995                  I am proud to be a hysterical woman\n",
      "8996    Hollywood is complicit in the rape and sexual ...\n",
      "8997    What a fucking cunt I hate seeing kids getting...\n",
      "8998                                Hysterical woman like\n",
      "8999    Nearly every woman I know has meToo in their f...\n",
      "Name: text, Length: 9000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Split x and ys\n",
    "x_train, y_train = x_y_split(train)\n",
    "print(x_train)\n",
    "x_train.to_csv('test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', lower=True)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "def tokenize_and_pad(x_data,tokenizer=tokenizer,length=57):\n",
    "    \"\"\"\n",
    "    Tokenizes and pads input\n",
    "    :param x_data: X column of data\n",
    "    :param tokenizer: fitted tokenizer\n",
    "    :param length: length to pad\n",
    "    :return: tokenized and padded x_data\n",
    "    \"\"\"\n",
    "    x_data = tokenizer.texts_to_sequences(x_data)\n",
    "    x_data = pad_sequences(x_data,maxlen=length)\n",
    "    return x_data\n",
    "x_train = tokenize_and_pad(x_train,tokenizer)\n",
    "def split_and_tokenize(data,tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "    Splits tokenizes and pads data\n",
    "    :param data:\n",
    "    :return: tupple of (X,y)\n",
    "    \"\"\"\n",
    "    X, y = x_y_split(data)\n",
    "    X = tokenize_and_pad(X)\n",
    "    return X, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "x_val , y_val = split_and_tokenize(val)\n",
    "x_test, y_test = split_and_tokenize(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "lstm_out = 392\n",
    "input_length=x_train.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "230718it [00:09, 23687.55it/s]"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/vsmolyakov/keras-cnn-with-fasttext-embeddings\n",
    "#load embeddings\n",
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open('data/wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#embedding matrix\n",
    "nb_words = min(max_features, len(tokenizer.word_index))\n",
    "words_not_found = []\n",
    "embedding_matrix = np.zeros((nb_words,embed_dim))\n",
    "word_index = tokenizer.word_index\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "REG_STRENGTH_l1 = 0\n",
    "REG_STRENGTH_l2 = 0.05\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features,embed_dim,input_length=input_length, weights=[embedding_matrix],trainable=False))\n",
    "model.add(LSTM(lstm_out,dropout=0.5, kernel_regularizer=L2(REG_STRENGTH_l2)))\n",
    "model.add(Dense(256,activation='relu', kernel_regularizer=L2(REG_STRENGTH_l2)))\n",
    "model.add(Dropout(.65))\n",
    "model.add(Dense(256,activation='relu', kernel_regularizer=L2(REG_STRENGTH_l2)))\n",
    "model.add(Dropout(.65))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss=BinaryCrossentropy(),optimizer=Adam(learning_rate=0.0001),metrics=['accuracy'])\n",
    "#print(model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filepath = 'checkpoints/test2.hdf5'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath,\n",
    "                                                 monitor='val_loss',\n",
    "                                                save_best_only=True,\n",
    "                                                 verbose=1,\n",
    "                                                mode='min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "history = model.fit(x=x_train,\n",
    "                    y=y_train,batch_size=256,\n",
    "                    validation_data=(x_val,y_val),\n",
    "                    epochs=100,\n",
    "                    callbacks=callbacks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_accuracy_loss_chart(history,epoch_count):\n",
    "    \"\"\"\n",
    "    Plots accuracy and losses for each epoch\n",
    "    :param history: history from fitting model\n",
    "    :param epoch_count: amount of epochs to plot\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    epochs = [i for i in range(epoch_count)]\n",
    "    fig , ax = plt.subplots(1,2)\n",
    "    train_acc = history.history['accuracy']\n",
    "    train_loss = history.history['loss']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    val_loss = history.history['val_loss']\n",
    "    fig.set_size_inches(20,10)\n",
    "    ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\n",
    "    ax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')\n",
    "    ax[0].set_title('Training & Validation Accuracy')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    ax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\n",
    "    ax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')\n",
    "    ax[1].set_title('Training & Validation Loss')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].set_ylabel(\"Training & Validation Loss\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_accuracy_loss_chart(history,100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.70      1740\n",
      "           1       0.57      0.53      0.55      1260\n",
      "\n",
      "    accuracy                           0.64      3000\n",
      "   macro avg       0.63      0.62      0.62      3000\n",
      "weighted avg       0.63      0.64      0.64      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "model = tf.keras.models.load_model('checkpoints/test2.hdf5')\n",
    "#print(val)\n",
    "pred = model.predict(x_test)\n",
    "pred = np.round(pred)\n",
    "#print(pred)\n",
    "print(classification_report(y_test,pred))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#model.save('checkpoints/full_fast_textmodel')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Node2vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "x_train, y_train = x_y_split(train)\n",
    "\n",
    "api_key_file = open('TagMeApiKey','r')\n",
    "api_key = api_key_file.read()\n",
    "api_key_file.close()\n",
    "json_list = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tag_me_api_call(text,api_key=api_key):\n",
    "    \"\"\"\n",
    "\n",
    "    :param api_key: key to access api\n",
    "    :param text: text to pass\n",
    "    :return: json returned from TagMe\n",
    "    \"\"\"\n",
    "    URL = 'https://tagme.d4science.org/tagme/tag'\n",
    "    PARAMS = {'text' : text, 'lang': 'en', 'gcube-token':api_key}\n",
    "    json = requests.post(URL,params=PARAMS)\n",
    "    return json.json()\n",
    "\n",
    "#json_list = x_train.apply(tag_me_api_call)\n",
    "#print(json_list)\n",
    "for i in  tqdm(range((len(x_train)))):\n",
    "    if len(x_train[i]) == 0:\n",
    "        json_list.append({})\n",
    "    else:\n",
    "        x = tag_me_api_call(x_train[i])\n",
    "        json_list.append(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "json_list = pd.DataFrame(json_list)\n",
    "json_list.to_csv('data/tag-me-train-spots.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [12:01<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('data/hateval2019_en_test.csv')\n",
    "val = pd.read_csv('data/hateval2019_en_dev.csv')\n",
    "x_val, y_val = x_y_split(val)\n",
    "x_test, y_test = x_y_split(test)\n",
    "\n",
    "json_list2 = []\n",
    "for i in  tqdm(range((len(x_val)))):\n",
    "    if len(x_val[i]) == 0:\n",
    "        json_list2.append({})\n",
    "    else:\n",
    "        x = tag_me_api_call(x_val[i])\n",
    "        json_list2.append(x)\n",
    "\n",
    "json_list2 = pd.DataFrame(json_list2)\n",
    "json_list2.to_csv('data/tag-me-val-spots.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "json_list3 = []\n",
    "for i in  tqdm(range((len(x_test)))):\n",
    "    if len(x_test[i]) == 0:\n",
    "        json_list3.append({})\n",
    "    else:\n",
    "        x = tag_me_api_call(x_test[i])\n",
    "        json_list3.append(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_spots = pd.DataFrame(json_list3)\n",
    "test_spots.to_csv('data/tag-me-test-spots.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(json_list3[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tag_me_api_call(x_val[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}