{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LD_LIBRARY_PATH=/ home / zach / anaconda3 / envs / research / lib\n"
     ]
    }
   ],
   "source": [
    "%env LD_LIBRARY_PATH= / home / zach / anaconda3 / envs / research / lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import codecs\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5856/962749627.py:6: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  train = train.drop(['TR', 'AG'], 1)\n",
      "/tmp/ipykernel_5856/962749627.py:7: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  test = test.drop(['TR', 'AG'], 1)\n",
      "/tmp/ipykernel_5856/962749627.py:8: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  val = val.drop(['TR', 'AG'], 1)\n"
     ]
    }
   ],
   "source": [
    "## Data Import and Cleaning\n",
    "train = pd.read_csv('data/hateval2019_en_train.csv')\n",
    "test = pd.read_csv('data/hateval2019_en_test.csv')\n",
    "val = pd.read_csv('data/hateval2019_en_dev.csv')\n",
    "\n",
    "train = train.drop(['TR', 'AG'], 1)\n",
    "test = test.drop(['TR', 'AG'], 1)\n",
    "val = val.drop(['TR', 'AG'], 1)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.regularizers import L1, L2, l1_l2\n",
    "import io\n",
    "\n",
    "\n",
    "def normalize_tweet(text):\n",
    "    \"\"\"\n",
    "    Removes hashtags, @s, links, and punctuation\n",
    "    :param text:Text to be cleaned\n",
    "    :return: text with mentions, hashtages, and urls removes\n",
    "    \"\"\"\n",
    "    processed_text = text.lower()\n",
    "    processed_text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www|t\\.)\\S+\", \"\", processed_text)\n",
    "    processed_text = re.sub(r\"(?:\\.|,|\\?|-)\", \" \", processed_text)\n",
    "    processed_text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www|\\.com)\", \"\", processed_text)\n",
    "    processed_text = re.sub(r'[^\\w\\s]', '', processed_text)\n",
    "    processed_text = \" \".join(processed_text.split())\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "def x_y_split(data):\n",
    "    \"\"\"splits and X and y from dataframe\n",
    "\n",
    "    Args:\n",
    "        data:dataframe to split from\n",
    "\n",
    "    Returns:\n",
    "        tuple:X data, y data\n",
    "    \"\"\"\n",
    "    X = data['text']\n",
    "    X = X.apply(normalize_tweet)\n",
    "    y = data['HS']\n",
    "    return X, y\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       hurray saving us in so many ways lockthemup bu...\n",
      "1       why would young fighting age men be the vast m...\n",
      "2       illegals dump their kids at the border like ro...\n",
      "3       ny times nearly all white states pose an array...\n",
      "4       orban in brussels european leaders are ignorin...\n",
      "                              ...                        \n",
      "8995                  i am proud to be a hysterical woman\n",
      "8996    hollywood is complicit in the rape and sexual ...\n",
      "8997    what a fucking cunt i hate seeing kids getting...\n",
      "8998                                hysterical woman like\n",
      "8999    nearly every woman i know has metoo in their f...\n",
      "Name: text, Length: 9000, dtype: object\n",
      "0       oh i could have gone on about taxes since the ...\n",
      "1       several of the wild fires in california and co...\n",
      "2       my question is how do you resettle a refugee a...\n",
      "3       europe youve got a problem we must hurry and b...\n",
      "4       this is outrageous stopillegalimmigration meri...\n",
      "                              ...                        \n",
      "2995    you can never take a l off a real bitch im hot...\n",
      "2996    likes to call me a cunt a bitch but i tell him...\n",
      "2997    1 never said you were taught 2 you called me b...\n",
      "2998    if i see and know you a hoe why would i hit yo...\n",
      "2999     you be chasing them hoes fuck what a bitch think\n",
      "Name: text, Length: 3000, dtype: object\n",
      "0      i swear im getting to places just in the nick ...\n",
      "1      im an immigrant and trump is right on immigrat...\n",
      "2      illegalimmigrants illegalaliens electoralsyste...\n",
      "3      we have our own invasion issues with mexicans ...\n",
      "4      worker charged with sexually molesting eight c...\n",
      "                             ...                        \n",
      "995                     you unfollowed me fuck you pussy\n",
      "996    stfu bitch and you go make some satanic music ...\n",
      "997    honey as a fellow white chick let me tell you ...\n",
      "998    i hate bitches who talk about niggaz with kids...\n",
      "999    you won the life time recipient for hysterical...\n",
      "Name: text, Length: 1000, dtype: object\n",
      "Number of unique words: 17392\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'the': 1,\n 'to': 2,\n 'a': 3,\n 'you': 4,\n 'and': 5,\n 'of': 6,\n 'in': 7,\n 'is': 8,\n 'for': 9,\n 'i': 10,\n 'are': 11,\n 'not': 12,\n 'that': 13,\n 'on': 14,\n 'bitch': 15,\n 'this': 16,\n 'it': 17,\n 'all': 18,\n 'your': 19,\n 'they': 20,\n 'be': 21,\n 'with': 22,\n 'refugees': 23,\n 'have': 24,\n 'women': 25,\n 'we': 26,\n 'me': 27,\n 'immigrant': 28,\n 'from': 29,\n 'when': 30,\n 'my': 31,\n 'like': 32,\n 'if': 33,\n 'immigration': 34,\n 'who': 35,\n 'dont': 36,\n 'but': 37,\n 'their': 38,\n 'no': 39,\n 'about': 40,\n 'so': 41,\n 'illegal': 42,\n 'as': 43,\n 'by': 44,\n 'will': 45,\n 'up': 46,\n 'at': 47,\n 'our': 48,\n 'just': 49,\n 'migrants': 50,\n 'do': 51,\n 'its': 52,\n 'what': 53,\n 'or': 54,\n 'men': 55,\n 'get': 56,\n 'people': 57,\n 'u': 58,\n 'them': 59,\n 'her': 60,\n 'an': 61,\n 'woman': 62,\n 'was': 63,\n 'can': 64,\n 'rape': 65,\n 'cunt': 66,\n 'how': 67,\n 'more': 68,\n 'out': 69,\n 'go': 70,\n 'whore': 71,\n 'trump': 72,\n 'immigrants': 73,\n 'one': 74,\n 'us': 75,\n 'has': 76,\n 'want': 77,\n 'fuck': 78,\n 'im': 79,\n 'youre': 80,\n 'why': 81,\n 'time': 82,\n 'he': 83,\n 'buildthatwall': 84,\n 'now': 85,\n 'she': 86,\n 'amp': 87,\n 'know': 88,\n 'should': 89,\n 'refugee': 90,\n 'ass': 91,\n 'country': 92,\n 'back': 93,\n 'stop': 94,\n 'hysterical': 95,\n 'via': 96,\n 'fucking': 97,\n 'hoe': 98,\n 'home': 99,\n 'only': 100,\n 'make': 101,\n 'being': 102,\n 'would': 103,\n 'his': 104,\n 'over': 105,\n 'girl': 106,\n 'need': 107,\n 'stupid': 108,\n 'here': 109,\n 'because': 110,\n 'right': 111,\n 'new': 112,\n 'take': 113,\n 'dick': 114,\n 'children': 115,\n 'some': 116,\n 'these': 117,\n 'say': 118,\n 'maga': 119,\n 'were': 120,\n 'into': 121,\n 'after': 122,\n 's': 123,\n 'there': 124,\n 'think': 125,\n 'migrant': 126,\n 'skank': 127,\n 'good': 128,\n 'never': 129,\n 'than': 130,\n 'then': 131,\n 'pussy': 132,\n 'shit': 133,\n 'europe': 134,\n 'free': 135,\n 'many': 136,\n 'see': 137,\n 'border': 138,\n 'eu': 139,\n 'cant': 140,\n 'been': 141,\n 'white': 142,\n 'world': 143,\n 'off': 144,\n 'too': 145,\n 'life': 146,\n 'man': 147,\n 'help': 148,\n 'buildthewall': 149,\n 'slut': 150,\n 'year': 151,\n 'come': 152,\n 'even': 153,\n 'other': 154,\n 'muslim': 155,\n 'hole': 156,\n 'must': 157,\n 'thats': 158,\n 'got': 159,\n '2': 160,\n 'first': 161,\n 'against': 162,\n 'any': 163,\n 'did': 164,\n 'those': 165,\n 'still': 166,\n 'down': 167,\n 'where': 168,\n 'love': 169,\n 'really': 170,\n 'let': 171,\n 'most': 172,\n 'support': 173,\n 'countries': 174,\n 'says': 175,\n 'way': 176,\n 'give': 177,\n 'illegals': 178,\n 'him': 179,\n 'migration': 180,\n 'had': 181,\n 'old': 182,\n 'call': 183,\n 'look': 184,\n 'please': 185,\n 'kids': 186,\n 'which': 187,\n '1': 188,\n 'said': 189,\n 'much': 190,\n 'shut': 191,\n 'american': 192,\n 'own': 193,\n 'also': 194,\n 'always': 195,\n 'welcome': 196,\n 'going': 197,\n 'womensuck': 198,\n 'families': 199,\n 'every': 200,\n 'america': 201,\n 'keep': 202,\n 'tell': 203,\n 'while': 204,\n 'well': 205,\n 'rights': 206,\n 'nodaca': 207,\n 'work': 208,\n 'ever': 209,\n 'hate': 210,\n 'wall': 211,\n 'someone': 212,\n 'today': 213,\n 'another': 214,\n 'same': 215,\n '000': 216,\n 'doesnt': 217,\n 'day': 218,\n 'speech': 219,\n 'last': 220,\n 'better': 221,\n 'watch': 222,\n 'could': 223,\n 'girls': 224,\n 'rt': 225,\n 'germany': 226,\n 'stay': 227,\n 'years': 228,\n 'money': 229,\n 'law': 230,\n 'government': 231,\n 'anti': 232,\n 'again': 233,\n 'theyre': 234,\n 'big': 235,\n 'care': 236,\n 'does': 237,\n 'usa': 238,\n 'before': 239,\n 'real': 240,\n 'didnt': 241,\n 'trying': 242,\n 'end': 243,\n 'child': 244,\n 'am': 245,\n 'vote': 246,\n 'americans': 247,\n 'uk': 248,\n 'open': 249,\n '3': 250,\n 'dumb': 251,\n 'cause': 252,\n 'bitches': 253,\n 'wants': 254,\n 'ice': 255,\n 'little': 256,\n 'family': 257,\n 'called': 258,\n 'around': 259,\n 'ram': 260,\n 'un': 261,\n 'president': 262,\n 'mass': 263,\n 'show': 264,\n 'parents': 265,\n 'great': 266,\n 'shes': 267,\n 'without': 268,\n 'talk': 269,\n 'put': 270,\n 'wont': 271,\n 'bad': 272,\n 'pay': 273,\n 'left': 274,\n 'away': 275,\n 'nothing': 276,\n 'hope': 277,\n 'syrian': 278,\n 'legal': 279,\n 'deport': 280,\n 'enough': 281,\n 'such': 282,\n 'ur': 283,\n 'anyone': 284,\n 'state': 285,\n 'stoptheinvasion': 286,\n 'find': 287,\n 'police': 288,\n 'noamnesty': 289,\n 'next': 290,\n 'aliens': 291,\n 'citizens': 292,\n 'oh': 293,\n 'italy': 294,\n 'sendthemback': 295,\n 'policy': 296,\n 'needs': 297,\n 'getting': 298,\n 'stfu': 299,\n 'two': 300,\n 'made': 301,\n 'yes': 302,\n 'laws': 303,\n 'alien': 304,\n 'sure': 305,\n 'news': 306,\n 'aint': 307,\n 'wanna': 308,\n 'welfare': 309,\n 'mean': 310,\n 'send': 311,\n 'start': 312,\n 'through': 313,\n 'use': 314,\n 'deported': 315,\n 'feel': 316,\n 'saying': 317,\n 'r': 318,\n 'believe': 319,\n 'lol': 320,\n 'thank': 321,\n 'things': 322,\n 'sex': 323,\n 'isnt': 324,\n 'live': 325,\n 'doing': 326,\n 'borders': 327,\n 'million': 328,\n 'daca': 329,\n 'calling': 330,\n 'guys': 331,\n 'black': 332,\n 'already': 333,\n 'story': 334,\n 'week': 335,\n 'german': 336,\n 'report': 337,\n '5': 338,\n 'ill': 339,\n 'yourself': 340,\n '4': 341,\n 'coming': 342,\n 'hard': 343,\n 'spain': 344,\n 'problem': 345,\n 'friends': 346,\n 'gonna': 347,\n 'return': 348,\n 'sexual': 349,\n 'yet': 350,\n 'wrong': 351,\n 'very': 352,\n 'rednationrising': 353,\n 'house': 354,\n 'since': 355,\n 'god': 356,\n 'crisis': 357,\n 'yall': 358,\n 'asylum': 359,\n 'lives': 360,\n 'deportthemall': 361,\n 'leave': 362,\n 'lets': 363,\n 'friend': 364,\n 'long': 365,\n 'face': 366,\n 'human': 367,\n 'anything': 368,\n 'suck': 369,\n 'cock': 370,\n 'hoes': 371,\n 'under': 372,\n 'everyone': 373,\n 'something': 374,\n 'democrats': 375,\n 'ive': 376,\n 'head': 377,\n 'workers': 378,\n 'place': 379,\n 'best': 380,\n 'guy': 381,\n 'hes': 382,\n 'rohingya': 383,\n 'x009d': 384,\n 'both': 385,\n 'crime': 386,\n 'become': 387,\n 'bring': 388,\n 'may': 389,\n 'culture': 390,\n 'talking': 391,\n 'young': 392,\n 'hell': 393,\n 'having': 394,\n 'person': 395,\n 'told': 396,\n 'living': 397,\n 'thing': 398,\n 'try': 399,\n 'between': 400,\n 'ugly': 401,\n 'actually': 402,\n 'id': 403,\n 'read': 404,\n 'thanks': 405,\n 'india': 406,\n 'job': 407,\n 'illegally': 408,\n 'done': 409,\n 'theres': 410,\n 'maybe': 411,\n 'accused': 412,\n 'video': 413,\n 'abuse': 414,\n 'protect': 415,\n 'community': 416,\n 'non': 417,\n 'true': 418,\n 'run': 419,\n 'learn': 420,\n 'stand': 421,\n 'w': 422,\n 'detention': 423,\n 'n': 424,\n 'political': 425,\n 'food': 426,\n 'once': 427,\n 'racist': 428,\n 'full': 429,\n 'change': 430,\n 'reason': 431,\n 'states': 432,\n 'nation': 433,\n 'canada': 434,\n 'part': 435,\n 'public': 436,\n 'poor': 437,\n 'using': 438,\n 'gets': 439,\n 'kunt': 440,\n 'mexico': 441,\n 'means': 442,\n '10': 443,\n 'city': 444,\n 'âž': 445,\n 'safe': 446,\n 'ya': 447,\n 'trash': 448,\n 'lil': 449,\n 'african': 450,\n 'fake': 451,\n 'media': 452,\n 'jobs': 453,\n 'working': 454,\n 'seen': 455,\n 'play': 456,\n 'texas': 457,\n 'death': 458,\n 'build': 459,\n 'making': 460,\n 'raped': 461,\n 'muslims': 462,\n 'understand': 463,\n 'whole': 464,\n 'killed': 465,\n 'nice': 466,\n 'fuckin': 467,\n 'came': 468,\n 'control': 469,\n 'obama': 470,\n '100': 471,\n 'looking': 472,\n 'fact': 473,\n 'point': 474,\n 'war': 475,\n '2018': 476,\n 'etc': 477,\n 'ok': 478,\n 'makes': 479,\n 'merkel': 480,\n 'hey': 481,\n 'fight': 482,\n 'instead': 483,\n 'deportation': 484,\n 'criminal': 485,\n 'less': 486,\n 'taking': 487,\n 'happy': 488,\n 'night': 489,\n 'ask': 490,\n 'mouth': 491,\n 'enddaca': 492,\n 'court': 493,\n 'plan': 494,\n 'judge': 495,\n 'withrefugees': 496,\n 'france': 497,\n 'party': 498,\n 'act': 499,\n 'twitter': 500,\n 'hear': 501,\n 'pro': 502,\n 'eat': 503,\n 'forced': 504,\n '13': 505,\n 'walkaway': 506,\n 'name': 507,\n 'side': 508,\n 'die': 509,\n 'thought': 510,\n 'mind': 511,\n 'housing': 512,\n 'respect': 513,\n 'youve': 514,\n 'economic': 515,\n 'hit': 516,\n 'allowed': 517,\n 'across': 518,\n 'national': 519,\n 'sweden': 520,\n 'lot': 521,\n 'security': 522,\n 'until': 523,\n 'crazy': 524,\n 'remember': 525,\n 'social': 526,\n 'bc': 527,\n 'speak': 528,\n 'fat': 529,\n 'yo': 530,\n 'ones': 531,\n 'truth': 532,\n 'issue': 533,\n 'system': 534,\n 'used': 535,\n 'follow': 536,\n 'check': 537,\n 'game': 538,\n 'school': 539,\n 'join': 540,\n 'victims': 541,\n 'kind': 542,\n 'syria': 543,\n 'post': 544,\n 'dead': 545,\n 'comes': 546,\n 'during': 547,\n 'assault': 548,\n 'nigga': 549,\n 'accept': 550,\n 'nobody': 551,\n 'wait': 552,\n 'libya': 553,\n 'homes': 554,\n 'probably': 555,\n 'damn': 556,\n 'millions': 557,\n 'least': 558,\n 'members': 559,\n 'deserve': 560,\n 'kag': 561,\n 'violence': 562,\n 'sick': 563,\n 'everything': 564,\n 'female': 565,\n 'deal': 566,\n 'might': 567,\n 'attention': 568,\n 'self': 569,\n 'protest': 570,\n 'boy': 571,\n 'easy': 572,\n 'goes': 573,\n 'hundreds': 574,\n 'power': 575,\n '6': 576,\n 'business': 577,\n 'matter': 578,\n 'case': 579,\n 'lmao': 580,\n 'each': 581,\n 'including': 582,\n 'alone': 583,\n 'separated': 584,\n 'sad': 585,\n 'far': 586,\n 'foreign': 587,\n 'wouldnt': 588,\n 'camps': 589,\n 'baby': 590,\n 'justice': 591,\n 'boys': 592,\n 'turn': 593,\n 'ho': 594,\n 'af': 595,\n 'kill': 596,\n 'times': 597,\n 'africa': 598,\n '8': 599,\n 'demand': 600,\n 'others': 601,\n 'son': 602,\n '7': 603,\n 'action': 604,\n 'islamic': 605,\n 'whats': 606,\n 'population': 607,\n 'criminals': 608,\n 'camp': 609,\n 'administration': 610,\n 'lost': 611,\n 'arent': 612,\n 'trumps': 613,\n 'close': 614,\n 'groups': 615,\n 'fear': 616,\n 'bill': 617,\n 'prison': 618,\n 'seekers': 619,\n 'military': 620,\n 'days': 621,\n 'christian': 622,\n 'liberals': 623,\n 'fucked': 624,\n 'cut': 625,\n 'aid': 626,\n 'past': 627,\n 'heart': 628,\n 'course': 629,\n 'tried': 630,\n 'thousands': 631,\n 'minister': 632,\n 'mother': 633,\n 'russia': 634,\n 'save': 635,\n 'found': 636,\n 'high': 637,\n 'given': 638,\n 'arrested': 639,\n 'either': 640,\n 'agree': 641,\n 'tells': 642,\n 'bangladesh': 643,\n 'order': 644,\n 'special': 645,\n 'together': 646,\n 'difference': 647,\n 'hungary': 648,\n 'spanish': 649,\n 'mad': 650,\n 'gotta': 651,\n 'whores': 652,\n 'cannot': 653,\n 'european': 654,\n 'able': 655,\n 'italian': 656,\n 'federal': 657,\n 'few': 658,\n 'paid': 659,\n 'allow': 660,\n 'gone': 661,\n 'killing': 662,\n '20': 663,\n 'move': 664,\n 'girlfriend': 665,\n 'line': 666,\n 'lying': 667,\n 'taxpayers': 668,\n 'giving': 669,\n 'communities': 670,\n 'group': 671,\n 'americafirst': 672,\n 'â': 673,\n 'undocumented': 674,\n 'went': 675,\n 'congress': 676,\n 'x008f': 677,\n 'center': 678,\n 'else': 679,\n 'dems': 680,\n 'share': 681,\n 'fun': 682,\n 'break': 683,\n 'started': 684,\n 'health': 685,\n 'sign': 686,\n 'question': 687,\n 'heard': 688,\n 'wife': 689,\n 'west': 690,\n 'month': 691,\n 'looks': 692,\n 'local': 693,\n 'tweet': 694,\n 'calls': 695,\n 'policies': 696,\n 'saw': 697,\n 'car': 698,\n 'literally': 699,\n 'yeah': 700,\n 'sorry': 701,\n 'dirty': 702,\n 'beat': 703,\n 'metoo': 704,\n 'education': 705,\n 'listen': 706,\n 'crimes': 707,\n 'islam': 708,\n 'south': 709,\n 'worker': 710,\n 'takes': 711,\n 'building': 712,\n 'drunk': 713,\n 'united': 714,\n 'asked': 715,\n 'tax': 716,\n 'waiting': 717,\n 'phone': 718,\n 'gives': 719,\n 'piece': 720,\n 'word': 721,\n 'months': 722,\n 'youll': 723,\n 'fire': 724,\n 'finally': 725,\n 'leaders': 726,\n 'heres': 727,\n 'enter': 728,\n '30': 729,\n 'catch': 730,\n 'disgusting': 731,\n 'charged': 732,\n 'murder': 733,\n 'invasion': 734,\n 'myanmar': 735,\n 'number': 736,\n 'cities': 737,\n 'book': 738,\n 'red': 739,\n 'guess': 740,\n 'voters': 741,\n 'program': 742,\n 'hot': 743,\n 'wanted': 744,\n 'visegrad': 745,\n 'v4': 746,\n 'status': 747,\n 'broke': 748,\n 'services': 749,\n 'force': 750,\n 'realize': 751,\n 'middle': 752,\n 'idea': 753,\n 'strong': 754,\n 'b': 755,\n 'knows': 756,\n 'yesallmen': 757,\n 'notallmen': 758,\n 'taken': 759,\n 'though': 760,\n 'wish': 761,\n 'sexually': 762,\n 'army': 763,\n 'illegalaliens': 764,\n 'important': 765,\n 'daily': 766,\n 'separation': 767,\n 'jail': 768,\n 'ago': 769,\n 'benefits': 770,\n 'married': 771,\n 'pass': 772,\n 'sometimes': 773,\n 'relief': 774,\n 'campaign': 775,\n 'works': 776,\n 'due': 777,\n 'mom': 778,\n 'sleep': 779,\n 'attack': 780,\n 'citizenship': 781,\n 'hand': 782,\n 'fighting': 783,\n 'joke': 784,\n 'streets': 785,\n 'future': 786,\n 'labor': 787,\n 'stories': 788,\n 'israel': 789,\n 'account': 790,\n 'raise': 791,\n 'third': 792,\n 'familiesbelongtogether': 793,\n 'fall': 794,\n 'destroy': 795,\n 'facebook': 796,\n 'entire': 797,\n 'sent': 798,\n 'wake': 799,\n 'threat': 800,\n '12': 801,\n 'office': 802,\n 'set': 803,\n 'british': 804,\n 'endchainmigration': 805,\n 'ha': 806,\n 'date': 807,\n 'victim': 808,\n 'pretty': 809,\n 'rich': 810,\n 'continue': 811,\n 'detained': 812,\n 'politics': 813,\n 'nations': 814,\n 'claim': 815,\n 'rescue': 816,\n 'cost': 817,\n 'becoming': 818,\n 'soon': 819,\n 'govt': 820,\n 'buy': 821,\n 'mr': 822,\n 'gop': 823,\n 'peace': 824,\n 'palestinian': 825,\n 'california': 826,\n 'land': 827,\n 'thinks': 828,\n 'centers': 829,\n 'sound': 830,\n 'block': 831,\n 'wow': 832,\n 'liberal': 833,\n 'shame': 834,\n 'turkey': 835,\n 'bout': 836,\n 'kitchen': 837,\n 'pic': 838,\n 'language': 839,\n 'international': 840,\n 'english': 841,\n '500': 842,\n 'homeless': 843,\n 'potus': 844,\n 'e': 845,\n 'invaders': 846,\n 'gave': 847,\n 'funny': 848,\n 'article': 849,\n 'almost': 850,\n 'wonder': 851,\n 'based': 852,\n 'wages': 853,\n 'murdered': 854,\n 'mexican': 855,\n 'genocide': 856,\n 'greece': 857,\n 'whos': 858,\n 'walk': 859,\n 'died': 860,\n 'jordan': 861,\n 'low': 862,\n 'ðÿ': 863,\n 'supporting': 864,\n 'running': 865,\n 'door': 866,\n 'telling': 867,\n 'forget': 868,\n 'treat': 869,\n 'tip': 870,\n 'hurt': 871,\n 'pimp': 872,\n 'niggas': 873,\n 'ladies': 874,\n 'plans': 875,\n 'behind': 876,\n 'cuz': 877,\n 'turned': 878,\n '3rd': 879,\n 'delhi': 880,\n 'defend': 881,\n 'unitednations': 882,\n 'sharia': 883,\n 'diversity': 884,\n 'massive': 885,\n 'born': 886,\n 'hollywood': 887,\n 'answer': 888,\n 'visit': 889,\n 'wear': 890,\n 'pm': 891,\n 'voted': 892,\n 'three': 893,\n 'wasnt': 894,\n 'race': 895,\n 'amazing': 896,\n 'imagine': 897,\n 'civil': 898,\n 'crossing': 899,\n 'history': 900,\n 'watching': 901,\n 'pregnant': 902,\n 'violent': 903,\n 'release': 904,\n 'rules': 905,\n 'church': 906,\n 'shelter': 907,\n 'visa': 908,\n 'society': 909,\n 'ban': 910,\n 'charge': 911,\n 'democrat': 912,\n 'russian': 913,\n 'color': 914,\n 'access': 915,\n 'numbers': 916,\n 'austria': 917,\n 'male': 918,\n 'christians': 919,\n 'ucanews': 920,\n 'bank': 921,\n '2017': 922,\n 'single': 923,\n 'ppl': 924,\n 'term': 925,\n 'gang': 926,\n '25': 927,\n 'song': 928,\n 't': 929,\n 'stopped': 930,\n '16': 931,\n '50': 932,\n 'drop': 933,\n 'tweets': 934,\n 'weinstein': 935,\n 'nelly': 936,\n 'growing': 937,\n 'attacks': 938,\n 'french': 939,\n 'list': 940,\n 'tired': 941,\n 'paying': 942,\n 'helping': 943,\n 'per': 944,\n 'buildthedamnwall': 945,\n 'central': 946,\n 'shot': 947,\n 'vacation': 948,\n 'economy': 949,\n 'boat': 950,\n 'beautiful': 951,\n 'politicians': 952,\n '11': 953,\n '40': 954,\n 'near': 955,\n 'brexit': 956,\n 'unless': 957,\n 'fleeing': 958,\n 'global': 959,\n 'evil': 960,\n 'complete': 961,\n 'hold': 962,\n 'despite': 963,\n 'win': 964,\n 'whatever': 965,\n 'whose': 966,\n 'front': 967,\n 'cop': 968,\n 'sense': 969,\n 'enforcement': 970,\n 'pathetic': 971,\n 'tf': 972,\n 'cum': 973,\n 'ex': 974,\n 'themselves': 975,\n 'reports': 976,\n 'trust': 977,\n 'donald': 978,\n 'sendthemhome': 979,\n 'rapists': 980,\n 'rest': 981,\n 'western': 982,\n 'decide': 983,\n 'cross': 984,\n 'rapefugee': 985,\n 'morning': 986,\n 'britain': 987,\n 'second': 988,\n 'lie': 989,\n 'drug': 990,\n 'drugs': 991,\n 'dm': 992,\n 'blame': 993,\n 'north': 994,\n 'officials': 995,\n 'officer': 996,\n 'seriously': 997,\n 'meet': 998,\n 'reality': 999,\n 'fund': 1000,\n ...}"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Split sequences into train, validation, and test sets\n",
    "#Split x and ys\n",
    "x_train, y_train = x_y_split(train)\n",
    "print(x_train)\n",
    "#x_train.to_csv('data/x_train.csv')\n",
    "#y_train.to_csv('data/y_train.csv')\n",
    "#Split x and ys\n",
    "x_test, y_test = x_y_split(test)\n",
    "print(x_test)\n",
    "#x_test.to_csv('data/x_test.csv')\n",
    "#y_test.to_csv('data/y_test.csv')\n",
    "#Split x and ys\n",
    "x_val, y_val = x_y_split(val)\n",
    "print(x_val)\n",
    "#x_val.to_csv('data/x_val.csv')\n",
    "#y_val.to_csv('data/y_val.csv')\n",
    "## Tokenizer\n",
    "max_features = 15000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', lower=True)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "\n",
    "def tokenize_and_pad(x_data, tokenizer=tokenizer, length=57):\n",
    "    \"\"\"\n",
    "    Tokenizes and pads input\n",
    "    :param x_data: X column of data\n",
    "    :param tokenizer: fitted tokenizer\n",
    "    :param length: length to pad\n",
    "    :return: tokenized and padded x_data\n",
    "    \"\"\"\n",
    "    x_data = tokenizer.texts_to_sequences(x_data)\n",
    "    x_data = pad_sequences(x_data, maxlen=length)\n",
    "    return x_data\n",
    "\n",
    "\n",
    "x_train = tokenize_and_pad(x_train, tokenizer)\n",
    "\n",
    "\n",
    "def split_and_tokenize(data, tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "    Splits tokenizes and pads data\n",
    "    :param data:\n",
    "    :return: tupple of (X,y)\n",
    "    \"\"\"\n",
    "    X, y = x_y_split(data)\n",
    "    X = tokenize_and_pad(X)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "x_val, y_val = split_and_tokenize(val)\n",
    "x_test, y_test = split_and_tokenize(test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Number of unique words:\", len(word_index))\n",
    "word_index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917495it [01:20, 23911.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1917495 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17392/17392 [00:00<00:00, 634749.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of null word embeddings: 2456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Embedding Matrix using Wikipedia Embeddings\n",
    "#download Wikipedia 2014 embeddings from https://github.com/stanfordnlp/GloVe\n",
    "#Load GLoVe embeddings; here I use embeddings with only 100 dimensions\n",
    "embeddings_index = {}\n",
    "f = codecs.open('data/glove.42B.300d.txt', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))\n",
    "\n",
    "embeddings_index\n",
    "embed_dim = 300\n",
    "\n",
    "nb_words = min(max_features, len(tokenizer.word_index))\n",
    "words_not_found = []\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "word_index = tokenizer.word_index\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "train\n",
      "val\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "print('running')\n",
    "x_graph_train = pd.read_csv('data/extracted_annotations_train.csv')\n",
    "print('train')\n",
    "\n",
    "x_graph_val = pd.read_csv('data/extracted_annotations_val.csv')\n",
    "print('val')\n",
    "\n",
    "x_graph_test = pd.read_csv('data/extracted_annotations_test.csv')\n",
    "print('test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "x_graph_train = x_graph_train['annotations']\n",
    "x_graph_val = x_graph_val['annotations']\n",
    "x_graph_test = x_graph_test['annotations']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "x_graph_train = x_graph_train.apply(str)\n",
    "x_graph_val = x_graph_val.apply(str)\n",
    "x_graph_test = x_graph_test.apply(str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "max_features_graph = 15000\n",
    "graph_tokenizer = Tokenizer(num_words=max_features_graph,split=' ', lower=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "graph_tokenizer.fit_on_texts(x_graph_train)\n",
    "max_graph_length = 2254"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "x_graph_train = tokenize_and_pad(x_graph_train,graph_tokenizer,max_graph_length)\n",
    "x_graph_val = tokenize_and_pad(x_graph_val,graph_tokenizer,max_graph_length)\n",
    "x_graph_test = tokenize_and_pad(x_graph_test,graph_tokenizer,max_graph_length)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def get_embedding_matrix(tokenizer=graph_tokenizer,max_features=max_features_graph):\n",
    "    embeddings_index = {}\n",
    "    f = codecs.open('data/glove.42B.300d.txt', encoding='utf-8')\n",
    "    for line in tqdm(f):\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('found %s word vectors' % len(embeddings_index))\n",
    "\n",
    "    embeddings_index\n",
    "    embed_dim = 300\n",
    "\n",
    "    nb_words = min(max_features, len(tokenizer.word_index))\n",
    "    words_not_found = []\n",
    "    embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "    word_index = tokenizer.word_index\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            words_not_found.append(word)\n",
    "    print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    return embedding_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917495it [01:20, 23827.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1917495 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62807/62807 [00:00<00:00, 1842269.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of null word embeddings: 238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "graph_embbeding_matrix = get_embedding_matrix()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[    0,     0,     0, ..., 12249,     5,     1],\n       [    0,     0,     0, ...,     4,    82,   575],\n       [    0,     0,     0, ...,  3266,  1589,   117],\n       ...,\n       [    0,     0,     0, ...,  1362,     8,  2784],\n       [    0,     0,     0, ...,    88,    49,   389],\n       [    0,     0,     0, ...,     1,    34,    33]], dtype=int32)"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_graph_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "lstm_out = 392\n",
    "max_length = 57\n",
    "input_graph = Input(shape=(max_graph_length,))\n",
    "embbeding_graph =  Embedding(max_features_graph,embed_dim,input_length=max_graph_length, weights=[graph_embbeding_matrix],trainable=False)(input_graph)\n",
    "lstm_graph = LSTM(lstm_out,dropout=0.6)(embbeding_graph)\n",
    "graph_model = Model(inputs=input_graph,outputs =lstm_graph)\n",
    "\n",
    "\n",
    "input_x = Input(shape=(max_length,))\n",
    "embedding = Embedding(max_features,embed_dim,input_length=max_length, weights=[embedding_matrix],trainable=False)(input_x)\n",
    "lstm = LSTM(lstm_out,dropout=0.6)(embedding)\n",
    "normal_model = Model(inputs=input_x,outputs =lstm)\n",
    "\n",
    "combined = concatenate([normal_model.output,graph_model.output])\n",
    "\n",
    "x = Dense(256,activation='relu')(combined)\n",
    "x = Dropout(.6)(x)\n",
    "x = Dense(256,activation='relu')(x)\n",
    "x = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[normal_model.input,graph_model.input],outputs=x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 15:19:16.093399: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 54s 177ms/step - loss: 0.6441 - accuracy: 0.6259 - val_loss: 0.6823 - val_accuracy: 0.6180\n",
      "Epoch 2/5\n",
      "282/282 [==============================] - 50s 177ms/step - loss: 0.5746 - accuracy: 0.7061 - val_loss: 0.6267 - val_accuracy: 0.6740\n",
      "Epoch 3/5\n",
      "282/282 [==============================] - 50s 179ms/step - loss: 0.5537 - accuracy: 0.7187 - val_loss: 0.5811 - val_accuracy: 0.6820\n",
      "Epoch 4/5\n",
      "282/282 [==============================] - 51s 180ms/step - loss: 0.5414 - accuracy: 0.7312 - val_loss: 0.6127 - val_accuracy: 0.6720\n",
      "Epoch 5/5\n",
      "282/282 [==============================] - 51s 180ms/step - loss: 0.5364 - accuracy: 0.7338 - val_loss: 0.5629 - val_accuracy: 0.6880\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=BinaryCrossentropy(),optimizer=Adam(learning_rate=0.0001),metrics=['accuracy'])\n",
    "history = model.fit(x=[x_train,x_graph_train],\n",
    "                    y=y_train,batch_size=32,\n",
    "                    validation_data=([x_val,x_graph_val],y_val),\n",
    "                    epochs=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}