{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LD_LIBRARY_PATH=/home/zach/anaconda3/envs/research/lib\n"
     ]
    }
   ],
   "source": [
    "%env LD_LIBRARY_PATH=/home/zach/anaconda3/envs/research/lib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import codecs\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_264563/675437588.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  train = train.drop(['TR','AG'],1)\n",
      "/tmp/ipykernel_264563/675437588.py:6: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  test = test.drop(['TR','AG'],1)\n",
      "/tmp/ipykernel_264563/675437588.py:7: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  val = val.drop(['TR','AG'],1)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/hateval2019_en_train.csv')\n",
    "test = pd.read_csv('data/hateval2019_en_test.csv')\n",
    "val = pd.read_csv('data/hateval2019_en_dev.csv')\n",
    "\n",
    "train = train.drop(['TR','AG'],1)\n",
    "test = test.drop(['TR','AG'],1)\n",
    "val = val.drop(['TR','AG'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 11:53:31.301186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 11:53:31.319815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 11:53:31.320044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "    id                                               text  HS\n0  201  Hurray, saving us $$$ in so many ways @potus @...   1\n1  202  Why would young fighting age men be the vast m...   1\n2  203  @KamalaHarris Illegals Dump their Kids at the ...   1\n3  204  NY Times: 'Nearly All White' States Pose 'an A...   0\n4  205  Orban in Brussels: European leaders are ignori...   0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>HS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>201</td>\n      <td>Hurray, saving us $$$ in so many ways @potus @...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>202</td>\n      <td>Why would young fighting age men be the vast m...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>203</td>\n      <td>@KamalaHarris Illegals Dump their Kids at the ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>204</td>\n      <td>NY Times: 'Nearly All White' States Pose 'an A...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>205</td>\n      <td>Orban in Brussels: European leaders are ignori...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.regularizers import L1,L2, l1_l2\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_tweet(text):\n",
    "    \"\"\"\n",
    "    Removes hashtags, @s, links, and punctuation\n",
    "    :param text:Text to be cleaned\n",
    "    :return: text with mentions, hashtages, and urls removes\n",
    "    \"\"\"\n",
    "    processed_text = text.lower()\n",
    "    processed_text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www|t\\.)\\S+\", \"\", processed_text)\n",
    "    processed_text = re.sub(r\"(?:\\.|,|\\?|-)\", \" \", processed_text)\n",
    "    processed_text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www|\\.com)\", \"\", processed_text)\n",
    "    processed_text = re.sub(r'[^\\w\\s]', '', processed_text)\n",
    "    processed_text = \" \".join(processed_text.split())\n",
    "    return processed_text\n",
    "\n",
    "def x_y_split(data):\n",
    "    \"\"\"splits and X and y from dataframe\n",
    "\n",
    "    Args:\n",
    "        data:dataframe to split from\n",
    "\n",
    "    Returns:\n",
    "        tuple:X data, y data\n",
    "    \"\"\"\n",
    "    X = data['text']\n",
    "    X = X.apply(normalize_tweet)\n",
    "    y = data['HS']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split sequences into train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       hurray saving us in so many ways lockthemup bu...\n",
      "1       why would young fighting age men be the vast m...\n",
      "2       illegals dump their kids at the border like ro...\n",
      "3       ny times nearly all white states pose an array...\n",
      "4       orban in brussels european leaders are ignorin...\n",
      "                              ...                        \n",
      "8995                  i am proud to be a hysterical woman\n",
      "8996    hollywood is complicit in the rape and sexual ...\n",
      "8997    what a fucking cunt i hate seeing kids getting...\n",
      "8998                                hysterical woman like\n",
      "8999    nearly every woman i know has metoo in their f...\n",
      "Name: text, Length: 9000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Split x and ys\n",
    "x_train, y_train = x_y_split(train)\n",
    "print(x_train)\n",
    "#x_train.to_csv('data/x_train.csv')\n",
    "#y_train.to_csv('data/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       oh i could have gone on about taxes since the ...\n",
      "1       several of the wild fires in california and co...\n",
      "2       my question is how do you resettle a refugee a...\n",
      "3       europe youve got a problem we must hurry and b...\n",
      "4       this is outrageous stopillegalimmigration meri...\n",
      "                              ...                        \n",
      "2995    you can never take a l off a real bitch im hot...\n",
      "2996    likes to call me a cunt a bitch but i tell him...\n",
      "2997    1 never said you were taught 2 you called me b...\n",
      "2998    if i see and know you a hoe why would i hit yo...\n",
      "2999     you be chasing them hoes fuck what a bitch think\n",
      "Name: text, Length: 3000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Split x and ys\n",
    "x_test, y_test = x_y_split(test)\n",
    "print(x_test)\n",
    "#x_test.to_csv('data/x_test.csv')\n",
    "#y_test.to_csv('data/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      i swear im getting to places just in the nick ...\n",
      "1      im an immigrant and trump is right on immigrat...\n",
      "2      illegalimmigrants illegalaliens electoralsyste...\n",
      "3      we have our own invasion issues with mexicans ...\n",
      "4      worker charged with sexually molesting eight c...\n",
      "                             ...                        \n",
      "995                     you unfollowed me fuck you pussy\n",
      "996    stfu bitch and you go make some satanic music ...\n",
      "997    honey as a fellow white chick let me tell you ...\n",
      "998    i hate bitches who talk about niggaz with kids...\n",
      "999    you won the life time recipient for hysterical...\n",
      "Name: text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Split x and ys\n",
    "x_val, y_val = x_y_split(val)\n",
    "print(x_val)\n",
    "#x_val.to_csv('data/x_val.csv')\n",
    "#y_val.to_csv('data/y_val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 15000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', lower=True)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "\n",
    "def tokenize_and_pad(x_data, tokenizer=tokenizer, length=57):\n",
    "    \"\"\"\n",
    "    Tokenizes and pads input\n",
    "    :param x_data: X column of data\n",
    "    :param tokenizer: fitted tokenizer\n",
    "    :param length: length to pad\n",
    "    :return: tokenized and padded x_data\n",
    "    \"\"\"\n",
    "    x_data = tokenizer.texts_to_sequences(x_data)\n",
    "    x_data = pad_sequences(x_data, maxlen=length)\n",
    "    return x_data\n",
    "\n",
    "\n",
    "x_train = tokenize_and_pad(x_train, tokenizer)\n",
    "\n",
    "\n",
    "def split_and_tokenize(data, tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "    Splits tokenizes and pads data\n",
    "    :param data:\n",
    "    :return: tupple of (X,y)\n",
    "    \"\"\"\n",
    "    X, y = x_y_split(data)\n",
    "    X = tokenize_and_pad(X)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "x_val, y_val = split_and_tokenize(val)\n",
    "x_test, y_test = split_and_tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 17392\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(\"Number of unique words:\", len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'the': 1,\n 'to': 2,\n 'a': 3,\n 'you': 4,\n 'and': 5,\n 'of': 6,\n 'in': 7,\n 'is': 8,\n 'for': 9,\n 'i': 10,\n 'are': 11,\n 'not': 12,\n 'that': 13,\n 'on': 14,\n 'bitch': 15,\n 'this': 16,\n 'it': 17,\n 'all': 18,\n 'your': 19,\n 'they': 20,\n 'be': 21,\n 'with': 22,\n 'refugees': 23,\n 'have': 24,\n 'women': 25,\n 'we': 26,\n 'me': 27,\n 'immigrant': 28,\n 'from': 29,\n 'when': 30,\n 'my': 31,\n 'like': 32,\n 'if': 33,\n 'immigration': 34,\n 'who': 35,\n 'dont': 36,\n 'but': 37,\n 'their': 38,\n 'no': 39,\n 'about': 40,\n 'so': 41,\n 'illegal': 42,\n 'as': 43,\n 'by': 44,\n 'will': 45,\n 'up': 46,\n 'at': 47,\n 'our': 48,\n 'just': 49,\n 'migrants': 50,\n 'do': 51,\n 'its': 52,\n 'what': 53,\n 'or': 54,\n 'men': 55,\n 'get': 56,\n 'people': 57,\n 'u': 58,\n 'them': 59,\n 'her': 60,\n 'an': 61,\n 'woman': 62,\n 'was': 63,\n 'can': 64,\n 'rape': 65,\n 'cunt': 66,\n 'how': 67,\n 'more': 68,\n 'out': 69,\n 'go': 70,\n 'whore': 71,\n 'trump': 72,\n 'immigrants': 73,\n 'one': 74,\n 'us': 75,\n 'has': 76,\n 'want': 77,\n 'fuck': 78,\n 'im': 79,\n 'youre': 80,\n 'why': 81,\n 'time': 82,\n 'he': 83,\n 'buildthatwall': 84,\n 'now': 85,\n 'she': 86,\n 'amp': 87,\n 'know': 88,\n 'should': 89,\n 'refugee': 90,\n 'ass': 91,\n 'country': 92,\n 'back': 93,\n 'stop': 94,\n 'hysterical': 95,\n 'via': 96,\n 'fucking': 97,\n 'hoe': 98,\n 'home': 99,\n 'only': 100,\n 'make': 101,\n 'being': 102,\n 'would': 103,\n 'his': 104,\n 'over': 105,\n 'girl': 106,\n 'need': 107,\n 'stupid': 108,\n 'here': 109,\n 'because': 110,\n 'right': 111,\n 'new': 112,\n 'take': 113,\n 'dick': 114,\n 'children': 115,\n 'some': 116,\n 'these': 117,\n 'say': 118,\n 'maga': 119,\n 'were': 120,\n 'into': 121,\n 'after': 122,\n 's': 123,\n 'there': 124,\n 'think': 125,\n 'migrant': 126,\n 'skank': 127,\n 'good': 128,\n 'never': 129,\n 'than': 130,\n 'then': 131,\n 'pussy': 132,\n 'shit': 133,\n 'europe': 134,\n 'free': 135,\n 'many': 136,\n 'see': 137,\n 'border': 138,\n 'eu': 139,\n 'cant': 140,\n 'been': 141,\n 'white': 142,\n 'world': 143,\n 'off': 144,\n 'too': 145,\n 'life': 146,\n 'man': 147,\n 'help': 148,\n 'buildthewall': 149,\n 'slut': 150,\n 'year': 151,\n 'come': 152,\n 'even': 153,\n 'other': 154,\n 'muslim': 155,\n 'hole': 156,\n 'must': 157,\n 'thats': 158,\n 'got': 159,\n '2': 160,\n 'first': 161,\n 'against': 162,\n 'any': 163,\n 'did': 164,\n 'those': 165,\n 'still': 166,\n 'down': 167,\n 'where': 168,\n 'love': 169,\n 'really': 170,\n 'let': 171,\n 'most': 172,\n 'support': 173,\n 'countries': 174,\n 'says': 175,\n 'way': 176,\n 'give': 177,\n 'illegals': 178,\n 'him': 179,\n 'migration': 180,\n 'had': 181,\n 'old': 182,\n 'call': 183,\n 'look': 184,\n 'please': 185,\n 'kids': 186,\n 'which': 187,\n '1': 188,\n 'said': 189,\n 'much': 190,\n 'shut': 191,\n 'american': 192,\n 'own': 193,\n 'also': 194,\n 'always': 195,\n 'welcome': 196,\n 'going': 197,\n 'womensuck': 198,\n 'families': 199,\n 'every': 200,\n 'america': 201,\n 'keep': 202,\n 'tell': 203,\n 'while': 204,\n 'well': 205,\n 'rights': 206,\n 'nodaca': 207,\n 'work': 208,\n 'ever': 209,\n 'hate': 210,\n 'wall': 211,\n 'someone': 212,\n 'today': 213,\n 'another': 214,\n 'same': 215,\n '000': 216,\n 'doesnt': 217,\n 'day': 218,\n 'speech': 219,\n 'last': 220,\n 'better': 221,\n 'watch': 222,\n 'could': 223,\n 'girls': 224,\n 'rt': 225,\n 'germany': 226,\n 'stay': 227,\n 'years': 228,\n 'money': 229,\n 'law': 230,\n 'government': 231,\n 'anti': 232,\n 'again': 233,\n 'theyre': 234,\n 'big': 235,\n 'care': 236,\n 'does': 237,\n 'usa': 238,\n 'before': 239,\n 'real': 240,\n 'didnt': 241,\n 'trying': 242,\n 'end': 243,\n 'child': 244,\n 'am': 245,\n 'vote': 246,\n 'americans': 247,\n 'uk': 248,\n 'open': 249,\n '3': 250,\n 'dumb': 251,\n 'cause': 252,\n 'bitches': 253,\n 'wants': 254,\n 'ice': 255,\n 'little': 256,\n 'family': 257,\n 'called': 258,\n 'around': 259,\n 'ram': 260,\n 'un': 261,\n 'president': 262,\n 'mass': 263,\n 'show': 264,\n 'parents': 265,\n 'great': 266,\n 'shes': 267,\n 'without': 268,\n 'talk': 269,\n 'put': 270,\n 'wont': 271,\n 'bad': 272,\n 'pay': 273,\n 'left': 274,\n 'away': 275,\n 'nothing': 276,\n 'hope': 277,\n 'syrian': 278,\n 'legal': 279,\n 'deport': 280,\n 'enough': 281,\n 'such': 282,\n 'ur': 283,\n 'anyone': 284,\n 'state': 285,\n 'stoptheinvasion': 286,\n 'find': 287,\n 'police': 288,\n 'noamnesty': 289,\n 'next': 290,\n 'aliens': 291,\n 'citizens': 292,\n 'oh': 293,\n 'italy': 294,\n 'sendthemback': 295,\n 'policy': 296,\n 'needs': 297,\n 'getting': 298,\n 'stfu': 299,\n 'two': 300,\n 'made': 301,\n 'yes': 302,\n 'laws': 303,\n 'alien': 304,\n 'sure': 305,\n 'news': 306,\n 'aint': 307,\n 'wanna': 308,\n 'welfare': 309,\n 'mean': 310,\n 'send': 311,\n 'start': 312,\n 'through': 313,\n 'use': 314,\n 'deported': 315,\n 'feel': 316,\n 'saying': 317,\n 'r': 318,\n 'believe': 319,\n 'lol': 320,\n 'thank': 321,\n 'things': 322,\n 'sex': 323,\n 'isnt': 324,\n 'live': 325,\n 'doing': 326,\n 'borders': 327,\n 'million': 328,\n 'daca': 329,\n 'calling': 330,\n 'guys': 331,\n 'black': 332,\n 'already': 333,\n 'story': 334,\n 'week': 335,\n 'german': 336,\n 'report': 337,\n '5': 338,\n 'ill': 339,\n 'yourself': 340,\n '4': 341,\n 'coming': 342,\n 'hard': 343,\n 'spain': 344,\n 'problem': 345,\n 'friends': 346,\n 'gonna': 347,\n 'return': 348,\n 'sexual': 349,\n 'yet': 350,\n 'wrong': 351,\n 'very': 352,\n 'rednationrising': 353,\n 'house': 354,\n 'since': 355,\n 'god': 356,\n 'crisis': 357,\n 'yall': 358,\n 'asylum': 359,\n 'lives': 360,\n 'deportthemall': 361,\n 'leave': 362,\n 'lets': 363,\n 'friend': 364,\n 'long': 365,\n 'face': 366,\n 'human': 367,\n 'anything': 368,\n 'suck': 369,\n 'cock': 370,\n 'hoes': 371,\n 'under': 372,\n 'everyone': 373,\n 'something': 374,\n 'democrats': 375,\n 'ive': 376,\n 'head': 377,\n 'workers': 378,\n 'place': 379,\n 'best': 380,\n 'guy': 381,\n 'hes': 382,\n 'rohingya': 383,\n 'x009d': 384,\n 'both': 385,\n 'crime': 386,\n 'become': 387,\n 'bring': 388,\n 'may': 389,\n 'culture': 390,\n 'talking': 391,\n 'young': 392,\n 'hell': 393,\n 'having': 394,\n 'person': 395,\n 'told': 396,\n 'living': 397,\n 'thing': 398,\n 'try': 399,\n 'between': 400,\n 'ugly': 401,\n 'actually': 402,\n 'id': 403,\n 'read': 404,\n 'thanks': 405,\n 'india': 406,\n 'job': 407,\n 'illegally': 408,\n 'done': 409,\n 'theres': 410,\n 'maybe': 411,\n 'accused': 412,\n 'video': 413,\n 'abuse': 414,\n 'protect': 415,\n 'community': 416,\n 'non': 417,\n 'true': 418,\n 'run': 419,\n 'learn': 420,\n 'stand': 421,\n 'w': 422,\n 'detention': 423,\n 'n': 424,\n 'political': 425,\n 'food': 426,\n 'once': 427,\n 'racist': 428,\n 'full': 429,\n 'change': 430,\n 'reason': 431,\n 'states': 432,\n 'nation': 433,\n 'canada': 434,\n 'part': 435,\n 'public': 436,\n 'poor': 437,\n 'using': 438,\n 'gets': 439,\n 'kunt': 440,\n 'mexico': 441,\n 'means': 442,\n '10': 443,\n 'city': 444,\n 'âž': 445,\n 'safe': 446,\n 'ya': 447,\n 'trash': 448,\n 'lil': 449,\n 'african': 450,\n 'fake': 451,\n 'media': 452,\n 'jobs': 453,\n 'working': 454,\n 'seen': 455,\n 'play': 456,\n 'texas': 457,\n 'death': 458,\n 'build': 459,\n 'making': 460,\n 'raped': 461,\n 'muslims': 462,\n 'understand': 463,\n 'whole': 464,\n 'killed': 465,\n 'nice': 466,\n 'fuckin': 467,\n 'came': 468,\n 'control': 469,\n 'obama': 470,\n '100': 471,\n 'looking': 472,\n 'fact': 473,\n 'point': 474,\n 'war': 475,\n '2018': 476,\n 'etc': 477,\n 'ok': 478,\n 'makes': 479,\n 'merkel': 480,\n 'hey': 481,\n 'fight': 482,\n 'instead': 483,\n 'deportation': 484,\n 'criminal': 485,\n 'less': 486,\n 'taking': 487,\n 'happy': 488,\n 'night': 489,\n 'ask': 490,\n 'mouth': 491,\n 'enddaca': 492,\n 'court': 493,\n 'plan': 494,\n 'judge': 495,\n 'withrefugees': 496,\n 'france': 497,\n 'party': 498,\n 'act': 499,\n 'twitter': 500,\n 'hear': 501,\n 'pro': 502,\n 'eat': 503,\n 'forced': 504,\n '13': 505,\n 'walkaway': 506,\n 'name': 507,\n 'side': 508,\n 'die': 509,\n 'thought': 510,\n 'mind': 511,\n 'housing': 512,\n 'respect': 513,\n 'youve': 514,\n 'economic': 515,\n 'hit': 516,\n 'allowed': 517,\n 'across': 518,\n 'national': 519,\n 'sweden': 520,\n 'lot': 521,\n 'security': 522,\n 'until': 523,\n 'crazy': 524,\n 'remember': 525,\n 'social': 526,\n 'bc': 527,\n 'speak': 528,\n 'fat': 529,\n 'yo': 530,\n 'ones': 531,\n 'truth': 532,\n 'issue': 533,\n 'system': 534,\n 'used': 535,\n 'follow': 536,\n 'check': 537,\n 'game': 538,\n 'school': 539,\n 'join': 540,\n 'victims': 541,\n 'kind': 542,\n 'syria': 543,\n 'post': 544,\n 'dead': 545,\n 'comes': 546,\n 'during': 547,\n 'assault': 548,\n 'nigga': 549,\n 'accept': 550,\n 'nobody': 551,\n 'wait': 552,\n 'libya': 553,\n 'homes': 554,\n 'probably': 555,\n 'damn': 556,\n 'millions': 557,\n 'least': 558,\n 'members': 559,\n 'deserve': 560,\n 'kag': 561,\n 'violence': 562,\n 'sick': 563,\n 'everything': 564,\n 'female': 565,\n 'deal': 566,\n 'might': 567,\n 'attention': 568,\n 'self': 569,\n 'protest': 570,\n 'boy': 571,\n 'easy': 572,\n 'goes': 573,\n 'hundreds': 574,\n 'power': 575,\n '6': 576,\n 'business': 577,\n 'matter': 578,\n 'case': 579,\n 'lmao': 580,\n 'each': 581,\n 'including': 582,\n 'alone': 583,\n 'separated': 584,\n 'sad': 585,\n 'far': 586,\n 'foreign': 587,\n 'wouldnt': 588,\n 'camps': 589,\n 'baby': 590,\n 'justice': 591,\n 'boys': 592,\n 'turn': 593,\n 'ho': 594,\n 'af': 595,\n 'kill': 596,\n 'times': 597,\n 'africa': 598,\n '8': 599,\n 'demand': 600,\n 'others': 601,\n 'son': 602,\n '7': 603,\n 'action': 604,\n 'islamic': 605,\n 'whats': 606,\n 'population': 607,\n 'criminals': 608,\n 'camp': 609,\n 'administration': 610,\n 'lost': 611,\n 'arent': 612,\n 'trumps': 613,\n 'close': 614,\n 'groups': 615,\n 'fear': 616,\n 'bill': 617,\n 'prison': 618,\n 'seekers': 619,\n 'military': 620,\n 'days': 621,\n 'christian': 622,\n 'liberals': 623,\n 'fucked': 624,\n 'cut': 625,\n 'aid': 626,\n 'past': 627,\n 'heart': 628,\n 'course': 629,\n 'tried': 630,\n 'thousands': 631,\n 'minister': 632,\n 'mother': 633,\n 'russia': 634,\n 'save': 635,\n 'found': 636,\n 'high': 637,\n 'given': 638,\n 'arrested': 639,\n 'either': 640,\n 'agree': 641,\n 'tells': 642,\n 'bangladesh': 643,\n 'order': 644,\n 'special': 645,\n 'together': 646,\n 'difference': 647,\n 'hungary': 648,\n 'spanish': 649,\n 'mad': 650,\n 'gotta': 651,\n 'whores': 652,\n 'cannot': 653,\n 'european': 654,\n 'able': 655,\n 'italian': 656,\n 'federal': 657,\n 'few': 658,\n 'paid': 659,\n 'allow': 660,\n 'gone': 661,\n 'killing': 662,\n '20': 663,\n 'move': 664,\n 'girlfriend': 665,\n 'line': 666,\n 'lying': 667,\n 'taxpayers': 668,\n 'giving': 669,\n 'communities': 670,\n 'group': 671,\n 'americafirst': 672,\n 'â': 673,\n 'undocumented': 674,\n 'went': 675,\n 'congress': 676,\n 'x008f': 677,\n 'center': 678,\n 'else': 679,\n 'dems': 680,\n 'share': 681,\n 'fun': 682,\n 'break': 683,\n 'started': 684,\n 'health': 685,\n 'sign': 686,\n 'question': 687,\n 'heard': 688,\n 'wife': 689,\n 'west': 690,\n 'month': 691,\n 'looks': 692,\n 'local': 693,\n 'tweet': 694,\n 'calls': 695,\n 'policies': 696,\n 'saw': 697,\n 'car': 698,\n 'literally': 699,\n 'yeah': 700,\n 'sorry': 701,\n 'dirty': 702,\n 'beat': 703,\n 'metoo': 704,\n 'education': 705,\n 'listen': 706,\n 'crimes': 707,\n 'islam': 708,\n 'south': 709,\n 'worker': 710,\n 'takes': 711,\n 'building': 712,\n 'drunk': 713,\n 'united': 714,\n 'asked': 715,\n 'tax': 716,\n 'waiting': 717,\n 'phone': 718,\n 'gives': 719,\n 'piece': 720,\n 'word': 721,\n 'months': 722,\n 'youll': 723,\n 'fire': 724,\n 'finally': 725,\n 'leaders': 726,\n 'heres': 727,\n 'enter': 728,\n '30': 729,\n 'catch': 730,\n 'disgusting': 731,\n 'charged': 732,\n 'murder': 733,\n 'invasion': 734,\n 'myanmar': 735,\n 'number': 736,\n 'cities': 737,\n 'book': 738,\n 'red': 739,\n 'guess': 740,\n 'voters': 741,\n 'program': 742,\n 'hot': 743,\n 'wanted': 744,\n 'visegrad': 745,\n 'v4': 746,\n 'status': 747,\n 'broke': 748,\n 'services': 749,\n 'force': 750,\n 'realize': 751,\n 'middle': 752,\n 'idea': 753,\n 'strong': 754,\n 'b': 755,\n 'knows': 756,\n 'yesallmen': 757,\n 'notallmen': 758,\n 'taken': 759,\n 'though': 760,\n 'wish': 761,\n 'sexually': 762,\n 'army': 763,\n 'illegalaliens': 764,\n 'important': 765,\n 'daily': 766,\n 'separation': 767,\n 'jail': 768,\n 'ago': 769,\n 'benefits': 770,\n 'married': 771,\n 'pass': 772,\n 'sometimes': 773,\n 'relief': 774,\n 'campaign': 775,\n 'works': 776,\n 'due': 777,\n 'mom': 778,\n 'sleep': 779,\n 'attack': 780,\n 'citizenship': 781,\n 'hand': 782,\n 'fighting': 783,\n 'joke': 784,\n 'streets': 785,\n 'future': 786,\n 'labor': 787,\n 'stories': 788,\n 'israel': 789,\n 'account': 790,\n 'raise': 791,\n 'third': 792,\n 'familiesbelongtogether': 793,\n 'fall': 794,\n 'destroy': 795,\n 'facebook': 796,\n 'entire': 797,\n 'sent': 798,\n 'wake': 799,\n 'threat': 800,\n '12': 801,\n 'office': 802,\n 'set': 803,\n 'british': 804,\n 'endchainmigration': 805,\n 'ha': 806,\n 'date': 807,\n 'victim': 808,\n 'pretty': 809,\n 'rich': 810,\n 'continue': 811,\n 'detained': 812,\n 'politics': 813,\n 'nations': 814,\n 'claim': 815,\n 'rescue': 816,\n 'cost': 817,\n 'becoming': 818,\n 'soon': 819,\n 'govt': 820,\n 'buy': 821,\n 'mr': 822,\n 'gop': 823,\n 'peace': 824,\n 'palestinian': 825,\n 'california': 826,\n 'land': 827,\n 'thinks': 828,\n 'centers': 829,\n 'sound': 830,\n 'block': 831,\n 'wow': 832,\n 'liberal': 833,\n 'shame': 834,\n 'turkey': 835,\n 'bout': 836,\n 'kitchen': 837,\n 'pic': 838,\n 'language': 839,\n 'international': 840,\n 'english': 841,\n '500': 842,\n 'homeless': 843,\n 'potus': 844,\n 'e': 845,\n 'invaders': 846,\n 'gave': 847,\n 'funny': 848,\n 'article': 849,\n 'almost': 850,\n 'wonder': 851,\n 'based': 852,\n 'wages': 853,\n 'murdered': 854,\n 'mexican': 855,\n 'genocide': 856,\n 'greece': 857,\n 'whos': 858,\n 'walk': 859,\n 'died': 860,\n 'jordan': 861,\n 'low': 862,\n 'ðÿ': 863,\n 'supporting': 864,\n 'running': 865,\n 'door': 866,\n 'telling': 867,\n 'forget': 868,\n 'treat': 869,\n 'tip': 870,\n 'hurt': 871,\n 'pimp': 872,\n 'niggas': 873,\n 'ladies': 874,\n 'plans': 875,\n 'behind': 876,\n 'cuz': 877,\n 'turned': 878,\n '3rd': 879,\n 'delhi': 880,\n 'defend': 881,\n 'unitednations': 882,\n 'sharia': 883,\n 'diversity': 884,\n 'massive': 885,\n 'born': 886,\n 'hollywood': 887,\n 'answer': 888,\n 'visit': 889,\n 'wear': 890,\n 'pm': 891,\n 'voted': 892,\n 'three': 893,\n 'wasnt': 894,\n 'race': 895,\n 'amazing': 896,\n 'imagine': 897,\n 'civil': 898,\n 'crossing': 899,\n 'history': 900,\n 'watching': 901,\n 'pregnant': 902,\n 'violent': 903,\n 'release': 904,\n 'rules': 905,\n 'church': 906,\n 'shelter': 907,\n 'visa': 908,\n 'society': 909,\n 'ban': 910,\n 'charge': 911,\n 'democrat': 912,\n 'russian': 913,\n 'color': 914,\n 'access': 915,\n 'numbers': 916,\n 'austria': 917,\n 'male': 918,\n 'christians': 919,\n 'ucanews': 920,\n 'bank': 921,\n '2017': 922,\n 'single': 923,\n 'ppl': 924,\n 'term': 925,\n 'gang': 926,\n '25': 927,\n 'song': 928,\n 't': 929,\n 'stopped': 930,\n '16': 931,\n '50': 932,\n 'drop': 933,\n 'tweets': 934,\n 'weinstein': 935,\n 'nelly': 936,\n 'growing': 937,\n 'attacks': 938,\n 'french': 939,\n 'list': 940,\n 'tired': 941,\n 'paying': 942,\n 'helping': 943,\n 'per': 944,\n 'buildthedamnwall': 945,\n 'central': 946,\n 'shot': 947,\n 'vacation': 948,\n 'economy': 949,\n 'boat': 950,\n 'beautiful': 951,\n 'politicians': 952,\n '11': 953,\n '40': 954,\n 'near': 955,\n 'brexit': 956,\n 'unless': 957,\n 'fleeing': 958,\n 'global': 959,\n 'evil': 960,\n 'complete': 961,\n 'hold': 962,\n 'despite': 963,\n 'win': 964,\n 'whatever': 965,\n 'whose': 966,\n 'front': 967,\n 'cop': 968,\n 'sense': 969,\n 'enforcement': 970,\n 'pathetic': 971,\n 'tf': 972,\n 'cum': 973,\n 'ex': 974,\n 'themselves': 975,\n 'reports': 976,\n 'trust': 977,\n 'donald': 978,\n 'sendthemhome': 979,\n 'rapists': 980,\n 'rest': 981,\n 'western': 982,\n 'decide': 983,\n 'cross': 984,\n 'rapefugee': 985,\n 'morning': 986,\n 'britain': 987,\n 'second': 988,\n 'lie': 989,\n 'drug': 990,\n 'drugs': 991,\n 'dm': 992,\n 'blame': 993,\n 'north': 994,\n 'officials': 995,\n 'officer': 996,\n 'seriously': 997,\n 'meet': 998,\n 'reality': 999,\n 'fund': 1000,\n ...}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix using Wikipedia Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "531532it [00:21, 24429.64it/s]"
     ]
    }
   ],
   "source": [
    "#download Wikipedia 2014 embeddings from https://github.com/stanfordnlp/GloVe\n",
    "#Load GLoVe embeddings; here I use embeddings with only 100 dimensions\n",
    "embeddings_index = {}\n",
    "f = codecs.open('data/glove.42B.300d.txt', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "\n",
    "nb_words = min(max_features, len(tokenizer.word_index))\n",
    "words_not_found = []\n",
    "embedding_matrix = np.zeros((nb_words,embed_dim))\n",
    "word_index = tokenizer.word_index\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filepath = 'checkpoints/glove_model.hdf5'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath,\n",
    "                                                 monitor='val_accuracy',\n",
    "                                                save_best_only=True,\n",
    "                                                 verbose=1,)\n",
    "callbacks = [checkpoint]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "f1 = tfa.metrics.F1Score(num_classes=1, average=None)\n",
    "input_length=x_train.shape[1]\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def __init__(self,embedding_matrix=embedding_matrix,max_features=max_features,embed_dim=embed_dim,input_length=input_length):\n",
    "        self.embedding_matrix= embedding_matrix\n",
    "        self.max_features = max_features\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_length = input_length\n",
    "\n",
    "    def build(self, hp):\n",
    "        lr = hp.Float(\"lr\", min_value=1e-5, max_value=1e-2, sampling=\"log\")\n",
    "        dropoutLSTM = hp.Float('dropoutLSTM',min_value=0,max_value=1)\n",
    "        dropout1 = hp.Float('dropout1',min_value=0,max_value=1)\n",
    "        dropout2 = hp.Float('dropout2',min_value=0,max_value=1)\n",
    "        wd = hp.Choice('wd', [0.0,0.01,0.001,0.1,0.005,0.05,.00001,.0001])\n",
    "        l2_1 = hp.Choice('l2_1', [0.0,0.01,0.001,0.1,0.005,0.05,.00001,.0001])\n",
    "        l2_2 = hp.Choice('l2_2', [0.0,0.01,0.001,0.1,0.005,0.05,.00001,.0001])\n",
    "        l2_3 = hp.Choice('l2_3', [0.0,0.01,0.001,0.1,0.005,0.05,.00001,.0001])\n",
    "        lstm_size = hp.Choice('lstm_size', [128,256,512,1024])\n",
    "        dense_1_size = hp.Choice('dense_1_size', [128,256,512,1024])\n",
    "        dense_2_size = hp.Choice('dense_2_size', [128,256,512,1024])\n",
    "\n",
    "        optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n",
    "        #input_length=x_train.shape[1]\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self.max_features,self.embed_dim,input_length=self.input_length, weights=[self.embedding_matrix],trainable=False))\n",
    "        model.add(LSTM(lstm_size, dropout=dropoutLSTM,kernel_regularizer=L2(l2_1)))\n",
    "        model.add(Dense(dense_1_size,activation='relu', kernel_regularizer=L2(l2_2)))\n",
    "        model.add(Dropout(dropout1))\n",
    "        model.add(Dense(dense_2_size,activation='relu', kernel_regularizer=L2(l2_3)))\n",
    "        model.add(Dropout(dropout2))\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1,'accuracy'])\n",
    "        return model\n",
    "\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [64, 32, 128]),\n",
    "            epochs = 50,\n",
    "            verbose = 2,\n",
    "            **kwargs,\n",
    "        )\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    MyHyperModel(),\n",
    "    objective= kt.Objective('val_f1_score',direction='max'),\n",
    "    max_trials = 100,\n",
    "    directory=\"checkpoints\",\n",
    "    project_name=\"GloVe_search\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tuner.search(x=x_train,\n",
    "             y=y_train,\n",
    "             validation_data=(x_val,y_val))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "parameters = ['lr','dropoutLSTM','dropout1','dropout2','wd','l2_1','l2_2','l2_3','lstm_size','dense_1_size','dense_2_size',\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "values = {}\n",
    "\n",
    "for i in parameters:\n",
    "    values[i] = best_hps.get(i)\n",
    "\n",
    "values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = tuner.get_best_models()[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = model.predict(x_train)\n",
    "pred = np.round(pred)\n",
    "#print(pred)\n",
    "print(classification_report(y_train, pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = model.predict(x_val)\n",
    "pred = np.round(pred)\n",
    "#print(pred)\n",
    "print(classification_report(y_val, pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)\n",
    "pred = np.round(pred)\n",
    "#print(pred)\n",
    "print(classification_report(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}