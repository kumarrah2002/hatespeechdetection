{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0b5164-cce2-4873-a0c1-67be3505f777",
   "metadata": {},
   "source": [
    "# Annotation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e9be88-173e-44be-a847-c227060e620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2145498-77e2-49ea-8b8d-5ee2cd8750b1",
   "metadata": {},
   "source": [
    "# Method for normalizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f14fc6b4-6413-4098-9312-89b40f3aed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tweet(text):\n",
    "    \"\"\"\n",
    "    Removes hashtags, @s, links, and punctuation\n",
    "    :param text:Text to be cleaned\n",
    "    :return: text with mentions, hashtages, and urls removes\n",
    "    \"\"\"\n",
    "    processed_text = text.lower()\n",
    "    processed_text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www|t\\.)\\S+\", \"\", processed_text)\n",
    "    processed_text = re.sub(r\"(?:\\.|,|\\?|-)\", \" \", processed_text)\n",
    "    processed_text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www|\\.com)\", \"\", processed_text)\n",
    "    processed_text = re.sub(r'[^\\w\\s]', '', processed_text)\n",
    "    processed_text = \" \".join(processed_text.split())\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8894c59-0840-40f7-9d47-05e784a5b72c",
   "metadata": {},
   "source": [
    "# Method for extracting the abstract from a spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61c2bdba-7fcf-4050-b74c-c535e4ad5807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spot(text):\n",
    "    searchString = text\n",
    "    startWord = \"abstract\" \n",
    "    endWord = \"title\"\n",
    "    results = \"\"\n",
    "\n",
    "    index = 0\n",
    "    while True:\n",
    "        try:\n",
    "            startIndex = searchString.index(startWord, index)\n",
    "            endIndex = searchString.index(endWord, startIndex)\n",
    "\n",
    "            results += (searchString[startIndex + len(startWord):endIndex].strip()) + \". \"\n",
    "\n",
    "            # move the index to the end\n",
    "            index = endIndex + len(endWord)\n",
    "\n",
    "        except ValueError:\n",
    "            # str.index raises a ValueError if there is no match; in that\n",
    "            # case we know that weâ€™re done looking at the string, so we can\n",
    "            # break out of the loop\n",
    "            break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c7c7e07-3c46-437d-8a2c-39fc1d627bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/tag-me-train-spots.csv\")\n",
    "test = pd.read_csv(\"data/tag-me-test-spots.csv\")\n",
    "val = pd.read_csv(\"data/tag-me-val-spots.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76adfaef-1e63-4fe7-be92-1b9790b52d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turns each element in the annotations column for all three dataframes into a string\n",
    "\n",
    "train_annotations = train['annotations'].astype(str)\n",
    "test_annotations = test['annotations'].astype(str)\n",
    "val_annotations = val['annotations'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4edc3114-ea82-4a7e-afc0-4160fa0ed00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in range(0, len(train_annotations)):\n",
    "    train_annotations[string] = normalize_tweet(train_annotations[string])\n",
    "for string in range(0, len(test_annotations)):\n",
    "    test_annotations[string] = normalize_tweet(test_annotations[string])\n",
    "for string in range(0, len(val_annotations)):\n",
    "    val_annotations[string] = normalize_tweet(val_annotations[string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d336f79e-272f-4d06-814b-81ed04f7d40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 152\n",
      "534 542\n",
      "1358 1366\n",
      "2021 2029\n",
      "Count: 4\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for m in re.finditer('abstract', train_annotations[0]):\n",
    "    count += 1\n",
    "    print(m.start(), m.end())\n",
    "print(\"Count: \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab3f25f-b8b1-4ae5-a2e5-d697a560df87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 121179 abstract sentences\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for string in train_annotations:\n",
    "    for m in re.finditer('abstract', string):\n",
    "        count += 1\n",
    "for string in test_annotations:\n",
    "    for m in re.finditer('abstract', string):\n",
    "        count += 1\n",
    "for string in val_annotations:\n",
    "    for m in re.finditer('abstract', string):\n",
    "        count += 1\n",
    "print(\"Count: \" + str(count) + \" abstract sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57b85e2e-4a3b-43f6-8f8f-ac098ebb4880",
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in range(0, len(train_annotations)):\n",
    "    train_annotations[string] = extract_spot(train_annotations[string])\n",
    "for string in range(0, len(test_annotations)):\n",
    "    test_annotations[string] = extract_spot(test_annotations[string])\n",
    "for string in range(0, len(val_annotations)):\n",
    "    val_annotations[string] = extract_spot(val_annotations[string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b482b842-3c6c-4a49-b951-031e70d32b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations.to_csv('data/annotations_train.csv')\n",
    "test_annotations.to_csv('data/annotations_test.csv')\n",
    "val_annotations.to_csv('data/annotations_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62920231-4311-4c0f-a49a-05e179a67fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
